{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-tuning the top layers of a pre-trained network \n",
    "\n",
    "In this last part, we shall fine tune the top layers of the previous model, in order to adapt the model for the task of recognizing cats and dogs.\n",
    "\n",
    "As we know, the base layers of a CNN learn rudimentary features such as edges and corners, and the top most layers learn to detect high level features such as legs, face, ears etc. The aim is to fine-tune the top layers so that the CNN is able to learn high level features for just cats and dogs, rather than high level features of all the classes that the original VGG-16 was trained to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (5.2.0)\n",
      "/bin/sh: 1: python: not found\n"
     ]
    }
   ],
   "source": [
    "##This notebook is built around using tensorflow as the backend for keras\n",
    "!pip install pillow\n",
    "!KERAS_BACKEND=tensorflow python -c \"from keras import backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "import scipy\n",
    "import pylab as pl\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 50, 50\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fine-tuning the top layers of a pre-trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Start by instantiating the VGG base and loading its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", name=\"conv1_1\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", name=\"conv1_2\")`\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"conv2_1\")`\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"conv2_2\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3_1\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3_2\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3_3\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4_1\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4_2\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4_3\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5_1\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5_2\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5_3\")`\n"
     ]
    }
   ],
   "source": [
    "model_vgg = Sequential()\n",
    "model_vgg.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height,3)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('models/vgg/vgg16_weights.h5')\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(model_vgg.layers) - 1:\n",
    "        # we don't look at the last two layers in the savefile (fully-connected and activation)\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    layer = model_vgg.layers[k]\n",
    "\n",
    "    #if layer.__class__.__name__ in ['Convolution1D', 'Convolution2D', 'Convolution3D', 'AtrousConvolution2D']:\n",
    "    if weights:\n",
    "        weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n",
    "\n",
    "    layer.set_weights(weights)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classifier model to put on top of the convolutional model. For the fine tuning, we start with a fully trained-classifer. We will use the weights from the earlier model. And then we will add this model on top of the convolutional base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model_vgg.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "top_model.load_weights('models/bottleneck_40_epochs.h5')\n",
    "\n",
    "model_vgg.add(top_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine turning, we only want to train a few layers.  This line will set the first 25 layers (up to the conv block) to non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_vgg.layers[:25]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model_vgg.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1402 images belonging to 2 classes.\n",
      "Found 101 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., steps_per_epoch=43, epochs=20, validation_steps=100)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "43/43 [==============================] - 251s 6s/step - loss: 1.1474 - acc: 0.7589 - val_loss: 0.4342 - val_acc: 0.8317\n",
      "Epoch 2/20\n",
      "43/43 [==============================] - 246s 6s/step - loss: 0.5375 - acc: 0.8136 - val_loss: 0.3861 - val_acc: 0.8317\n",
      "Epoch 3/20\n",
      "43/43 [==============================] - 246s 6s/step - loss: 0.4373 - acc: 0.8428 - val_loss: 0.3859 - val_acc: 0.8614\n",
      "Epoch 4/20\n",
      "43/43 [==============================] - 246s 6s/step - loss: 0.3519 - acc: 0.8634 - val_loss: 0.3482 - val_acc: 0.8812\n",
      "Epoch 5/20\n",
      "43/43 [==============================] - 247s 6s/step - loss: 0.2979 - acc: 0.8762 - val_loss: 0.3666 - val_acc: 0.8614\n",
      "Epoch 6/20\n",
      "43/43 [==============================] - 247s 6s/step - loss: 0.2941 - acc: 0.8863 - val_loss: 0.3479 - val_acc: 0.8614\n",
      "Epoch 7/20\n",
      "43/43 [==============================] - 246s 6s/step - loss: 0.2542 - acc: 0.8961 - val_loss: 0.3495 - val_acc: 0.8713\n",
      "Epoch 8/20\n",
      "43/43 [==============================] - 246s 6s/step - loss: 0.2172 - acc: 0.9023 - val_loss: 0.3435 - val_acc: 0.8713\n",
      "Epoch 9/20\n",
      "43/43 [==============================] - 246s 6s/step - loss: 0.1826 - acc: 0.9263 - val_loss: 0.3518 - val_acc: 0.8713\n",
      "Epoch 10/20\n",
      "43/43 [==============================] - 245s 6s/step - loss: 0.1648 - acc: 0.9363 - val_loss: 0.3447 - val_acc: 0.8812\n",
      "Epoch 11/20\n",
      "43/43 [==============================] - 247s 6s/step - loss: 0.1465 - acc: 0.9411 - val_loss: 0.3573 - val_acc: 0.8812\n",
      "Epoch 12/20\n",
      "43/43 [==============================] - 245s 6s/step - loss: 0.1309 - acc: 0.9465 - val_loss: 0.3827 - val_acc: 0.8713\n",
      "Epoch 13/20\n",
      "43/43 [==============================] - 245s 6s/step - loss: 0.1429 - acc: 0.9406 - val_loss: 0.3543 - val_acc: 0.8812\n",
      "Epoch 14/20\n",
      "43/43 [==============================] - 247s 6s/step - loss: 0.1206 - acc: 0.9513 - val_loss: 0.3526 - val_acc: 0.8713\n",
      "Epoch 15/20\n",
      "43/43 [==============================] - 247s 6s/step - loss: 0.0911 - acc: 0.9619 - val_loss: 0.3696 - val_acc: 0.8812\n",
      "Epoch 16/20\n",
      "43/43 [==============================] - 247s 6s/step - loss: 0.0970 - acc: 0.9608 - val_loss: 0.3847 - val_acc: 0.8812\n",
      "Epoch 17/20\n",
      "43/43 [==============================] - 245s 6s/step - loss: 0.0831 - acc: 0.9670 - val_loss: 0.3904 - val_acc: 0.8614\n",
      "Epoch 18/20\n",
      "43/43 [==============================] - 247s 6s/step - loss: 0.0827 - acc: 0.9717 - val_loss: 0.3911 - val_acc: 0.8713\n",
      "Epoch 19/20\n",
      "43/43 [==============================] - 244s 6s/step - loss: 0.0736 - acc: 0.9735 - val_loss: 0.3844 - val_acc: 0.8911\n",
      "Epoch 20/20\n",
      "43/43 [==============================] - 247s 6s/step - loss: 0.0748 - acc: 0.9666 - val_loss: 0.3859 - val_acc: 0.8713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8271e56a0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_epoch = 20\n",
    "# fine-tune the model\n",
    "model_vgg.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=1400,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.save_weights('models/finetuning_20epochs_vgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.load_weights('models/finetuning_20epochs_vgg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3858799598902224, 0.8712871291849873]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg.evaluate_generator(validation_generator, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy = ~87 %\n",
    "\n",
    "As we can see, just a little fine-tuning yields an accuracy that of 87%! This is well over the baseline accuracy of ~ 80%, which is also stated in this [paper](http://xenon.stanford.edu/~pgolle/papers/dogcat.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
