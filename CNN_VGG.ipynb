{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "In this part, we shall use pretrained network VGG-16 to extract image features from the cats and dog images. Then we will use a simple Multilayer perceptron to classify the images using the above extracted features as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (5.2.0)\n",
      "/bin/sh: 1: python: not found\n"
     ]
    }
   ],
   "source": [
    "##This notebook is built around using tensorflow as the backend for keras\n",
    "!pip install pillow\n",
    "!KERAS_BACKEND=tensorflow python -c \"from keras import backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: keras\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "import scipy\n",
    "import pylab as pl\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inlineimport keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator,array_to_img, img_to_array,load_img\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 50, 50\n",
    "\n",
    "train_data_dir = 'data/train/'\n",
    "validation_data_dir = 'data/validation/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of training a convolutionnal neural network can be very time-consuming and require a lot of datas.  \n",
    "\n",
    "We can go beyond the previous models in terms of performance and efficiency by using a general-purpose, pre-trained image classifier.  This example uses VGG16, a model trained on the ImageNet dataset - which contains millions of images classified in 1000 categories. \n",
    "\n",
    "On top of it, we add a small multi-layer perceptron and we train it on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 Network\n",
    "\n",
    "The VGG-16 is a 16-layer network used by the VGG team in the ILSVRC-2014 competition. The network architecture and implementation details can be found in this [paper](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "Transfer learning allows us to deal with these scenarios by leveraging the already existing labeled data of some related task or domain. We try to store this knowledge gained in solving the source task in the source domain and apply it to our problem of interest. This knowledge can take on various forms depending on the data: it can pertain to how objects are composed to allow us to more easily identify novel objects; it can be with regard to the general words people use to express their opinions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16 model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", name=\"conv1_1\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", name=\"conv1_2\")`\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"conv2_1\")`\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"conv2_2\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3_1\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3_2\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3_3\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4_1\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4_2\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4_3\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5_1\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5_2\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5_3\")`\n"
     ]
    }
   ],
   "source": [
    "model_vgg = Sequential()\n",
    "model_vgg.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height,3)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading VGG16 weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note : the VGG16 weights file (~500MB) is not included in this repository. You can download from here :  \n",
    "https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('models/vgg/vgg16_weights.h5')\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(model_vgg.layers) - 1:\n",
    "        # we don't look at the last two layers in the savefile (fully-connected and activation)\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    layer = model_vgg.layers[k]\n",
    "\n",
    "    #if layer.__class__.__name__ in ['Convolution1D', 'Convolution2D', 'Convolution3D', 'AtrousConvolution2D']:\n",
    "    if weights:\n",
    "        weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n",
    "        #print(weights[0].shape)\n",
    "    layer.set_weights(weights)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the VGG16 model to process samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1402 images belonging to 2 classes.\n",
      "Found 101 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "imageDataGenerator = ImageDataGenerator()\n",
    "train_generator_bottleneck = imageDataGenerator.flow_from_directory(train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "validation_generator_bottleneck = imageDataGenerator.flow_from_directory(validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a long process, so we save the output of the VGG16 once and for all.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_train = model_vgg.predict_generator(train_generator_bottleneck, 1400)\n",
    "np.save(open('models/bottleneck_features_train.npy', 'wb'), bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_validation = model_vgg.predict_generator(validation_generator_bottleneck, 100)\n",
    "np.save(open('models/bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(open('models/bottleneck_features_train.npy', 'rb'))\n",
    "train_labels = np.array([0] * (1400 // 2) + [1] * (1400 // 2))\n",
    "\n",
    "validation_data = np.load(open('models/bottleneck_features_validation.npy', 'rb'))\n",
    "validation_labels = np.array([0] * (100 // 2) + [1] * (100 // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define and train the custom fully connected neural network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top = Sequential()\n",
    "model_top.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model_top.add(Dense(256, activation='relu'))\n",
    "model_top.add(Dropout(0.5))\n",
    "model_top.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_top.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 100 samples\n",
      "Epoch 1/40\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 7.9834 - acc: 0.5036 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/40\n",
      "1400/1400 [==============================] - 1s 811us/step - loss: 6.6480 - acc: 0.5786 - val_loss: 6.9835 - val_acc: 0.5600\n",
      "Epoch 3/40\n",
      "1400/1400 [==============================] - 1s 756us/step - loss: 3.9570 - acc: 0.7443 - val_loss: 3.1671 - val_acc: 0.7700\n",
      "Epoch 4/40\n",
      "1400/1400 [==============================] - 1s 762us/step - loss: 3.3398 - acc: 0.7836 - val_loss: 3.6541 - val_acc: 0.7700\n",
      "Epoch 5/40\n",
      "1400/1400 [==============================] - 1s 799us/step - loss: 2.6386 - acc: 0.8257 - val_loss: 4.4832 - val_acc: 0.7200\n",
      "Epoch 6/40\n",
      "1400/1400 [==============================] - 1s 809us/step - loss: 2.7758 - acc: 0.8186 - val_loss: 2.8786 - val_acc: 0.8100\n",
      "Epoch 7/40\n",
      "1400/1400 [==============================] - 1s 781us/step - loss: 2.5673 - acc: 0.8307 - val_loss: 2.9608 - val_acc: 0.8100\n",
      "Epoch 8/40\n",
      "1400/1400 [==============================] - 1s 773us/step - loss: 2.2191 - acc: 0.8557 - val_loss: 2.8581 - val_acc: 0.8200\n",
      "Epoch 9/40\n",
      "1400/1400 [==============================] - 1s 795us/step - loss: 2.3298 - acc: 0.8500 - val_loss: 2.7571 - val_acc: 0.8200\n",
      "Epoch 10/40\n",
      "1400/1400 [==============================] - 1s 788us/step - loss: 2.0520 - acc: 0.8657 - val_loss: 2.9250 - val_acc: 0.8100\n",
      "Epoch 11/40\n",
      "1400/1400 [==============================] - 1s 786us/step - loss: 2.0024 - acc: 0.8700 - val_loss: 2.7243 - val_acc: 0.8300\n",
      "Epoch 12/40\n",
      "1400/1400 [==============================] - 1s 804us/step - loss: 1.9181 - acc: 0.8729 - val_loss: 3.5407 - val_acc: 0.7800\n",
      "Epoch 13/40\n",
      "1400/1400 [==============================] - 1s 808us/step - loss: 2.0476 - acc: 0.8629 - val_loss: 3.3992 - val_acc: 0.7800\n",
      "Epoch 14/40\n",
      "1400/1400 [==============================] - 1s 793us/step - loss: 1.8360 - acc: 0.8814 - val_loss: 3.0021 - val_acc: 0.8100\n",
      "Epoch 15/40\n",
      "1400/1400 [==============================] - 1s 819us/step - loss: 1.9799 - acc: 0.8721 - val_loss: 3.8683 - val_acc: 0.7600\n",
      "Epoch 16/40\n",
      "1400/1400 [==============================] - 1s 830us/step - loss: 1.9290 - acc: 0.8786 - val_loss: 2.7746 - val_acc: 0.8200\n",
      "Epoch 17/40\n",
      "1400/1400 [==============================] - 1s 841us/step - loss: 1.6219 - acc: 0.8957 - val_loss: 2.7104 - val_acc: 0.8300\n",
      "Epoch 18/40\n",
      "1400/1400 [==============================] - 1s 792us/step - loss: 1.6188 - acc: 0.8964 - val_loss: 2.4305 - val_acc: 0.8400\n",
      "Epoch 19/40\n",
      "1400/1400 [==============================] - 1s 772us/step - loss: 1.4281 - acc: 0.9071 - val_loss: 2.8890 - val_acc: 0.8200\n",
      "Epoch 20/40\n",
      "1400/1400 [==============================] - 1s 777us/step - loss: 1.7583 - acc: 0.8850 - val_loss: 2.7261 - val_acc: 0.8300\n",
      "Epoch 21/40\n",
      "1400/1400 [==============================] - 1s 815us/step - loss: 1.4184 - acc: 0.9071 - val_loss: 2.7260 - val_acc: 0.8300\n",
      "Epoch 22/40\n",
      "1400/1400 [==============================] - 1s 866us/step - loss: 1.4680 - acc: 0.9050 - val_loss: 2.5759 - val_acc: 0.8300\n",
      "Epoch 23/40\n",
      "1400/1400 [==============================] - 1s 865us/step - loss: 1.6274 - acc: 0.8936 - val_loss: 2.8890 - val_acc: 0.8200\n",
      "Epoch 24/40\n",
      "1400/1400 [==============================] - 1s 886us/step - loss: 1.4604 - acc: 0.9036 - val_loss: 2.8949 - val_acc: 0.8200\n",
      "Epoch 25/40\n",
      "1400/1400 [==============================] - 1s 861us/step - loss: 1.6349 - acc: 0.8957 - val_loss: 2.7295 - val_acc: 0.8300\n",
      "Epoch 26/40\n",
      "1400/1400 [==============================] - 1s 838us/step - loss: 1.3853 - acc: 0.9121 - val_loss: 2.4142 - val_acc: 0.8500\n",
      "Epoch 27/40\n",
      "1400/1400 [==============================] - 1s 785us/step - loss: 1.5982 - acc: 0.8986 - val_loss: 2.5853 - val_acc: 0.8300\n",
      "Epoch 28/40\n",
      "1400/1400 [==============================] - 1s 777us/step - loss: 1.3539 - acc: 0.9150 - val_loss: 2.4078 - val_acc: 0.8500\n",
      "Epoch 29/40\n",
      "1400/1400 [==============================] - 1s 811us/step - loss: 1.1538 - acc: 0.9243 - val_loss: 3.0378 - val_acc: 0.8100\n",
      "Epoch 30/40\n",
      "1400/1400 [==============================] - 1s 748us/step - loss: 1.2909 - acc: 0.9164 - val_loss: 3.2061 - val_acc: 0.8000\n",
      "Epoch 31/40\n",
      "1400/1400 [==============================] - 1s 796us/step - loss: 1.7892 - acc: 0.8843 - val_loss: 2.7853 - val_acc: 0.8200\n",
      "Epoch 32/40\n",
      "1400/1400 [==============================] - 1s 797us/step - loss: 1.2864 - acc: 0.9200 - val_loss: 2.4443 - val_acc: 0.8400\n",
      "Epoch 33/40\n",
      "1400/1400 [==============================] - 1s 776us/step - loss: 1.2147 - acc: 0.9193 - val_loss: 2.5835 - val_acc: 0.8300\n",
      "Epoch 34/40\n",
      "1400/1400 [==============================] - 1s 846us/step - loss: 1.1056 - acc: 0.9264 - val_loss: 2.7420 - val_acc: 0.8200\n",
      "Epoch 35/40\n",
      "1400/1400 [==============================] - 1s 891us/step - loss: 1.0676 - acc: 0.9300 - val_loss: 2.7285 - val_acc: 0.8300\n",
      "Epoch 36/40\n",
      "1400/1400 [==============================] - 1s 822us/step - loss: 1.1038 - acc: 0.9293 - val_loss: 2.5655 - val_acc: 0.8400\n",
      "Epoch 37/40\n",
      "1400/1400 [==============================] - 1s 813us/step - loss: 0.9596 - acc: 0.9364 - val_loss: 2.7260 - val_acc: 0.8300\n",
      "Epoch 38/40\n",
      "1400/1400 [==============================] - 1s 828us/step - loss: 1.1348 - acc: 0.9271 - val_loss: 2.7243 - val_acc: 0.8300\n",
      "Epoch 39/40\n",
      "1400/1400 [==============================] - 1s 784us/step - loss: 1.1054 - acc: 0.9286 - val_loss: 2.4019 - val_acc: 0.8500\n",
      "Epoch 40/40\n",
      "1400/1400 [==============================] - 1s 828us/step - loss: 1.2294 - acc: 0.9193 - val_loss: 2.5596 - val_acc: 0.8400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f866b959ba8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=40\n",
    "model_top.fit(train_data[:1400], train_labels,\n",
    "          epochs=epochs, batch_size=32,\n",
    "          validation_data=(validation_data[:100], validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process of this small neural network is very fast : ~37s per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.save_weights('models/bottleneck_40_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.load_weights('models/bottleneck_40_epochs.h5')\n",
    "#model_top.load_weights('/notebook/Data1/Code/keras-workshop/models/with-bottleneck/1000-samples--100-epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 341us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5595671844482424, 0.84]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.evaluate(validation_data[:100], validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and accuracy :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We reached a 84% accuracy on the validation after 40 epochs, which is a massive boost on our baseline model!**\n",
    "\n",
    "Next, we shall try and finetune the model so that it adapts on this dataset, in order to further boost the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
